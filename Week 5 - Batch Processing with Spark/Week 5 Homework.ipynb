{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Object that we use for interacting with Spark, main entry point to spark \n",
    "# http://localhost:4040/jobs/ to see the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"fhvhv_tripdata_2021-02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.printSchema()\n",
    "\n",
    "#all strings, we don't want that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit the schema in Visual studio code, and convert to python\n",
    "\n",
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"fhvhv_tripdata_2021-02.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.repartition(24).write.parquet(\"hvfhv_2021-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6dd6af740a3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_fhv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df_fhv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.registerTempTable('FHV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read.parquet(\"hvfhv_2021-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# q3_df = df_fhv.filter( (df_fhv.pickup_datetime >= '2021-02-01 00:00:00') & (df_fhv.pickup_datetime) >= '2021-02-28 00:00:00').show()\n",
    "\n",
    "\n",
    "# //Filter multiple condition\n",
    "# df.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n",
    "#     .show(truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = spark.sql(\"\"\"\n",
    "SELECT * FROM FHV \n",
    "WHERE CAST(pickup_datetime as date) == '2021-02-15'\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-15 00:14:30|2021-02-15 00:26:11|         108|          22|   null|\n",
      "|           HV0005|              B02510|2021-02-15 00:23:46|2021-02-15 00:41:09|          37|          61|   null|\n",
      "|           HV0004|              B02800|2021-02-15 00:34:20|2021-02-15 00:39:34|          82|          82|   null|\n",
      "|           HV0004|              B02800|2021-02-15 00:49:51|2021-02-15 01:10:29|         138|         239|   null|\n",
      "|           HV0003|              B02884|2021-02-15 00:03:13|2021-02-15 00:19:23|          81|          47|   null|\n",
      "|           HV0003|              B02884|2021-02-15 00:33:30|2021-02-15 00:46:24|          69|         243|   null|\n",
      "|           HV0003|              B02884|2021-02-15 00:49:01|2021-02-15 00:54:45|         243|         243|   null|\n",
      "|           HV0003|              B02884|2021-02-15 00:55:56|2021-02-15 01:04:04|         127|         116|   null|\n",
      "|           HV0005|              B02510|2021-02-15 00:03:57|2021-02-15 00:13:38|          81|         240|   null|\n",
      "|           HV0005|              B02510|2021-02-15 00:39:03|2021-02-15 00:46:41|         259|         259|   null|\n",
      "|           HV0005|              B02510|2021-02-15 00:56:24|2021-02-15 01:01:35|         259|         265|   null|\n",
      "|           HV0003|              B02764|2021-02-15 00:22:13|2021-02-15 00:34:55|         254|         259|   null|\n",
      "|           HV0003|              B02875|2021-02-15 00:05:51|2021-02-15 00:15:17|          75|          42|   null|\n",
      "|           HV0003|              B02875|2021-02-15 00:18:19|2021-02-15 00:26:33|          42|          42|   null|\n",
      "|           HV0003|              B02875|2021-02-15 00:28:03|2021-02-15 00:35:30|          42|         244|   null|\n",
      "|           HV0003|              B02875|2021-02-15 00:39:05|2021-02-15 00:55:57|         120|          75|   null|\n",
      "|           HV0003|              B02875|2021-02-15 00:59:25|2021-02-15 01:04:08|          74|          74|   null|\n",
      "|           HV0005|              B02510|2021-02-15 00:48:58|2021-02-15 01:00:22|          69|         169|   null|\n",
      "|           HV0005|              B02510|2021-02-15 00:12:14|2021-02-15 00:25:58|         247|          32|   null|\n",
      "|           HV0005|              B02510|2021-02-15 00:44:15|2021-02-15 01:17:27|          31|          62|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_longest_trip = spark.sql(\"\"\"\n",
    "SELECT CAST(pickup_datetime as date) AS day, MAX((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime))/60) AS duration \n",
    "FROM FHV\n",
    "GROUP BY CAST(pickup_datetime as date)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|       day|          duration|\n",
      "+----------+------------------+\n",
      "|2021-02-15|431.23333333333335|\n",
      "|2021-02-02| 515.2166666666667|\n",
      "|2021-02-26|407.03333333333336|\n",
      "|2021-02-21|            537.05|\n",
      "|2021-02-05|508.51666666666665|\n",
      "|2021-02-10| 569.4833333333333|\n",
      "|2021-02-01|343.96666666666664|\n",
      "|2021-02-06| 524.1166666666667|\n",
      "|2021-02-19| 626.2833333333333|\n",
      "|2021-02-20| 733.9833333333333|\n",
      "|2021-02-08|501.76666666666665|\n",
      "|2021-02-09| 534.7833333333333|\n",
      "|2021-02-11|            1259.0|\n",
      "|2021-02-17| 953.6833333333333|\n",
      "|2021-02-25|             583.5|\n",
      "|2021-02-27| 452.8333333333333|\n",
      "|2021-02-24|394.48333333333335|\n",
      "|2021-02-18| 576.8666666666667|\n",
      "|2021-02-14|496.28333333333336|\n",
      "|2021-02-12|502.46666666666664|\n",
      "|2021-02-22|             471.3|\n",
      "|2021-02-03|            677.55|\n",
      "|2021-02-13| 357.3666666666667|\n",
      "|2021-02-23|407.31666666666666|\n",
      "|2021-02-16|424.01666666666665|\n",
      "|2021-02-07|294.53333333333336|\n",
      "|2021-02-28| 330.8333333333333|\n",
      "|2021-02-04|426.53333333333336|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day_longest_trip.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_dispatching_num = spark.sql(\"\"\"\n",
    "SELECT * FROM (\n",
    "SELECT dispatching_base_num, \n",
    "    COUNT(*) AS occurences,\n",
    "    RANK() OVER (ORDER BY COUNT(*) DESC) as rank\n",
    "FROM FHV \n",
    "GROUP BY dispatching_base_num)\n",
    "\n",
    "WHERE rank = 1\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----+\n",
      "|dispatching_base_num|occurences|rank|\n",
      "+--------------------+----------+----+\n",
      "|              B02510|   3233664|   1|\n",
      "+--------------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_frequent_dispatching_num.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df = spark.read.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_locations_pair = spark.sql(\"\"\"\n",
    "\n",
    "SELECT DISTINCT(CONCAT(COALESCE(zpu.Zone, \"Unknown\"), \" / \", COALESCE(zdo.Zone, \"Unknown\"))) AS pickup_and_dropoff,\n",
    "COUNT(*) as counts\n",
    "FROM \n",
    "    FHV LEFT JOIN zones zpu  ON\n",
    "        FHV.PULocationID = zpu.LocationID\n",
    "    LEFT JOIN zones zdo ON\n",
    "        FHV.DOLocationID = zdo.LocationID\n",
    "GROUP BY pickup_and_dropoff\n",
    "ORDER BY counts DESC\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Here we do two left joins, the Pick up location id on the left with the zones look up, \n",
    "# and Dropoff location id on the left with the look up table we give the lookup table a unique name for zpu and zdo\n",
    "# we use this in our SELECT(DISTINC(CONCAT)) Query so that we can get the string version of the name from the lookup table\n",
    "# then group by the pickup_and_dropoff \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SELECT MAX(avg_total) FROM (\n",
    "# SELECT \n",
    "# \tDISTINCT CONCAT(zpu.\"Zone\", ' / ', zdo.\"Zone\") AS pickup_and_dropoff, \n",
    "# \tAVG(total_amount) as avg_total\n",
    "# FROM \n",
    "# \tyellow_taxi_data y JOIN zones zpu \n",
    "# \t\tON y.\"PULocationID\" = zpu.\"LocationID\"\n",
    "# \tJOIN zones zdo\n",
    "# \t\tON y.\"DOLocationID\" = zdo.\"LocationID\"\n",
    "# GROUP BY pickup_and_dropoff\n",
    "# ) subquery;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|  pickup_and_dropoff|counts|\n",
      "+--------------------+------+\n",
      "|East New York / E...| 45041|\n",
      "|Borough Park / Bo...| 37329|\n",
      "| Canarsie / Canarsie| 28026|\n",
      "|Crown Heights Nor...| 25976|\n",
      "|Bay Ridge / Bay R...| 17934|\n",
      "|   Astoria / Astoria| 14688|\n",
      "|Jackson Heights /...| 14688|\n",
      "|Central Harlem No...| 14481|\n",
      "|Bushwick South / ...| 14424|\n",
      "|Flatbush/Ditmas P...| 13976|\n",
      "|South Ozone Park ...| 13716|\n",
      "|Brownsville / Bro...| 12829|\n",
      "|    JFK Airport / NA| 12542|\n",
      "|Prospect-Lefferts...| 11814|\n",
      "|Forest Hills / Fo...| 11548|\n",
      "|Bushwick North / ...| 11491|\n",
      "|Bushwick South / ...| 11487|\n",
      "|Crown Heights Nor...| 11462|\n",
      "|Crown Heights Nor...| 11342|\n",
      "|Prospect-Lefferts...| 11308|\n",
      "|Stuyvesant Height...| 11293|\n",
      "|Brownsville / Eas...| 11244|\n",
      "|   Bedford / Bedford| 11021|\n",
      "|Canarsie / East N...| 10688|\n",
      "|Stuyvesant Height...| 10675|\n",
      "|East New York / B...| 10621|\n",
      "| Elmhurst / Elmhurst| 10604|\n",
      "|Soundview/Castle ...| 10519|\n",
      "|Central Harlem No...| 10304|\n",
      "|Central Harlem / ...| 10260|\n",
      "|Bedford / Crown H...| 10185|\n",
      "|Crown Heights Nor...| 10181|\n",
      "|Washington Height...| 10165|\n",
      "|Woodlawn/Wakefiel...| 10091|\n",
      "|Park Slope / Park...|  9771|\n",
      "|East New York/Pen...|  9748|\n",
      "|  Astoria / Steinway|  9465|\n",
      "|Ridgewood / Ridge...|  9425|\n",
      "|East New York / C...|  9338|\n",
      "|Flatlands / Flatl...|  9331|\n",
      "+--------------------+------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_common_locations_pair.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
